{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: to build a model that predicts if a weapon was used or not based on the attributes of that crime\n",
    "\n",
    "This will use a binary outcome of true or false. I will try using bayesian model, logistic regression, random forrest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import numpy\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_crime_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure how many crime involved the use of a weapon. Lets look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.firearm_used_flag >=1]) # this feels like a cumbersome approach. Lets do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crimes where weapon was used\n",
      "+-----------+--------------------+\n",
      "|crime_count|         description|\n",
      "+-----------+--------------------+\n",
      "|         93|  Aggravated Assault|\n",
      "|         55|Aggravated Assaul...|\n",
      "|          8|Non Aggravated As...|\n",
      "|          6|       Armed Robbery|\n",
      "|          5|Non Aggravated As...|\n",
      "|          2|  Strong Arm Robbery|\n",
      "|          1|            Homocide|\n",
      "|          1|                Rape|\n",
      "|          1|Kidnapping/Abduction|\n",
      "+-----------+--------------------+\n",
      "\n",
      "all crimes\n",
      "+-----------+\n",
      "|crime_count|\n",
      "+-----------+\n",
      "|      30400|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "\n",
    "# create spark dataframes\n",
    "crime_df = spark.createDataFrame(df)\n",
    "\n",
    "crime_df.createOrReplaceTempView('crime')\n",
    "\n",
    "print('crimes where weapon was used')\n",
    "\n",
    "gun_crimes = spark.sql(\"\"\"\n",
    "select \n",
    "    count(distinct crime_id) as crime_count,\n",
    "    description\n",
    "from crime\n",
    "where firearm_used_flag >= 1\n",
    "and description not LIKE '%Weapons%'\n",
    "group by 2 order by 1 desc\n",
    "\"\"\")\n",
    "\n",
    "gun_crimes.show()\n",
    "\n",
    "print('all crimes')\n",
    "\n",
    "all_crimes = spark.sql(\"\"\"\n",
    "select\n",
    "count(distinct crime_id) as crime_count\n",
    "from crime\n",
    "\"\"\")\n",
    "\n",
    "all_crimes.show()\n",
    "\n",
    "#print('Weapons were used in {}% of the crimes in this data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weapons were used in 0.6% of the crimes in this data set\n"
     ]
    }
   ],
   "source": [
    "print('Weapons were used in {}% of the crimes in this data set'.format(round((gun_crimes.groupBy().sum().collect()[0][0]/\n",
    "                                                                            all_crimes.groupBy().sum().collect()[0][0]),3)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. This might seem pretty bad but actually there are a ton of types of crimes that we can exclude to narrow our focus and give this percentage a bit more of a fighting chance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = gun_crimes.toPandas()\n",
    "gc.drop([6], axis= 0, inplace = True)\n",
    "\n",
    "data = df[df.description.isin(gc.description.unique())]\n",
    "data.reset_index(inplace = True,drop = True)\n",
    "data = data.drop_duplicates(subset=['crime_id'], keep = False) \n",
    "data.firearm_used_flag = np.where(data.firearm_used_flag >= 1,1,0)\n",
    "data.dvflag = np.where(data.dvflag >= 1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now we have 2.37% of the crimes in this data set involving a firearm\n"
     ]
    }
   ],
   "source": [
    "print('now we have {}% of the crimes in this data set involving a firearm'.format(round((len(data[data.firearm_used_flag>=1])/\n",
    "                                                                                 len(data[data.firearm_used_flag<1]))*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will be much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naieve Bayes Classifer \n",
    "\n",
    "I will be using the Complement Naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Given that we are trying to predict an event that only occurs ~2% of the time, this is a good choice.\n",
    "\n",
    "https://www.youtube.com/watch?v=CPqOCI0ahss\n",
    "\n",
    "This is a really good video of explaining how a Naieve Bayes model works at a high level. Its really pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_no_gun</th>\n",
       "      <th>pred_gun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_no_gun</th>\n",
       "      <td>1842</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_gun</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred_no_gun  pred_gun\n",
       "actual_no_gun         1842       317\n",
       "actual_gun               8        48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# split the data, will use this same data for other models \n",
    "model_df = data.drop(columns=['crime_id','from_date','charge_id'])\n",
    "\n",
    "description = pd.get_dummies(model_df['description'])\n",
    "zipcode = pd.get_dummies(model_df['zip_code'])\n",
    "\n",
    "model_df_2 = pd.concat([model_df,description,zipcode], axis = 1)\n",
    "\n",
    "model_df_data = model_df_2.drop(columns=['firearm_used_flag','description','zip_code'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_df_data,model_df_2['firearm_used_flag'],test_size = .3,\n",
    "                                                    random_state = 42) # changing from .15\n",
    "# train the model\n",
    "model = ComplementNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# # put results to a confusion matrix\n",
    "nb_results = pd.DataFrame(confusion_matrix(y_test, predicted), columns=['pred_no_gun','pred_gun'],\n",
    "             index = ['actual_no_gun','actual_gun'])\n",
    "nb_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ok, this model feels alright. Lets break it down some:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelStats(results):\n",
    "    accuracy = ((results.loc['actual_no_gun','pred_no_gun'] +results.loc['actual_gun','pred_gun'])/results.values.sum())*100\n",
    "    mis_class = ((results.loc['actual_gun','pred_no_gun'] +results.loc['actual_no_gun','pred_gun'])/results.values.sum())*100\n",
    "    true_pos = ((results.loc['actual_gun','pred_gun']/results.loc['actual_gun'].sum()))*100\n",
    "    false_pos = ((results.loc['actual_no_gun','pred_gun']/results.loc['actual_no_gun'].sum()))*100\n",
    "    true_neg = ((results.loc['actual_no_gun','pred_no_gun']/results.pred_no_gun.sum()))*100\n",
    "    precision = ((results.loc['actual_gun','pred_gun']/results.pred_gun.sum()))*100\n",
    "    prevalence = (results.loc['actual_gun'].sum()/results.values.sum())*100\n",
    "\n",
    "    print('The model was {}% accuracte'.format(round(accuracy,2)))\n",
    "    print('The model had a misclassification rate of {}%'.format(round(mis_class,2)))\n",
    "    print('The model had a true positive rate of {}%'.format(round(true_pos,2)))\n",
    "    print('The model had a false positive rate of {}%'.format(round(false_pos,2)))\n",
    "    print('The model had a true negitive rate of {}%'.format(round(true_neg,2)))\n",
    "    print('The model had a precision rate of {}%'.format(round(precision,2)))\n",
    "    print('The model had a prevalence rate of {}%'.format(round(prevalence,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2159\n",
      "           1       0.13      0.86      0.23        56\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      2215\n",
      "   macro avg       0.56      0.86      0.57      2215\n",
      "weighted avg       0.97      0.85      0.90      2215\n",
      "\n",
      "The model was 85.33% accuracte\n",
      "The model had a misclassification rate of 14.67%\n",
      "The model had a true positive rate of 85.71%\n",
      "The model had a false positive rate of 14.68%\n",
      "The model had a true negitive rate of 99.57%\n",
      "The model had a precision rate of 13.15%\n",
      "The model had a prevalence rate of 2.53%\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted))\n",
    "print(modelStats(nb_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "Logistic regression used a logit function, which is basically a line that spans between 0 and 1. This is due to the formula for this line being 1/(1+e)^-z where e is Eulers number (2.71....) and z is a liner regression line (y=mx+b...) for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_no_gun</th>\n",
       "      <th>pred_gun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_no_gun</th>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_gun</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred_no_gun  pred_gun\n",
       "actual_no_gun         2159         0\n",
       "actual_gun              54         2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression(solver= 'liblinear')\n",
    "\n",
    "# using same data from the previous split\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "log_pred = logmodel.predict(X_test)\n",
    "\n",
    "log_results = pd.DataFrame(confusion_matrix(y_test, log_pred), columns=['pred_no_gun','pred_gun'],\n",
    "             index = ['actual_no_gun','actual_gun'])\n",
    "log_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2159\n",
      "           1       1.00      0.04      0.07        56\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2215\n",
      "   macro avg       0.99      0.52      0.53      2215\n",
      "weighted avg       0.98      0.98      0.96      2215\n",
      "\n",
      "The model was 97.56% accuracte\n",
      "The model had a misclassification rate of 2.44%\n",
      "The model had a true positive rate of 3.57%\n",
      "The model had a false positive rate of 0.0%\n",
      "The model had a true negitive rate of 97.56%\n",
      "The model had a precision rate of 100.0%\n",
      "The model had a prevalence rate of 2.53%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,log_pred))\n",
    "modelStats(log_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is pretty bad out of the box. In reality, you could get a decent score by just guessing no gun every single time which is kind of what happened here. Lets see if we can improve it before giving up completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False  True False False  True False\n",
      "  True  True False  True  True  True False False False False False False\n",
      " False False False False False False False  True  True False  True  True\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False]\n",
      "[11 31 37  3 52 88 16  2 18 39 84 36  5 19 45 56  1 61 46  1 42 41  1 23\n",
      "  1  1 65  1  1  1 75 76 87 89 66 71 91 79 49 73 64 69 60  1  1 47  1  1\n",
      " 24  1 17 15 44 55 14  8 25 57 20 67 63 34 32 10  6 21 50  1 22 27 62 33\n",
      " 26 78 43 35 12 48 74 28 51 53 81  1 40 38  4 29 13  7 30  9 59 58 86 85\n",
      " 70 90 82 83 80 77 68 72 92 54]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(logmodel,15)\n",
    "\n",
    "rfe.fit(X_train, y_train.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.concat([pd.DataFrame(X_train.columns, columns = ['name']), \n",
    "           pd.DataFrame(rfe.support_, columns = ['tf'])], axis = 1)\n",
    "feat = feat[feat.tf == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_no_gun</th>\n",
       "      <th>pred_gun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_no_gun</th>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_gun</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred_no_gun  pred_gun\n",
       "actual_no_gun         2159         0\n",
       "actual_gun              53         3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = feat.name.unique()\n",
    "X_train_rfe=X_train[cols]\n",
    "X_test_rfe = X_test[cols]\n",
    "\n",
    "\n",
    "logmodel.fit(X_train_rfe, y_train)\n",
    "\n",
    "log_pred = logmodel.predict(X_test_rfe)\n",
    "\n",
    "log_results = pd.DataFrame(confusion_matrix(y_test, log_pred), columns=['pred_no_gun','pred_gun'],\n",
    "             index = ['actual_no_gun','actual_gun'])\n",
    "log_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, still a bad model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      2159\n",
      "           1       0.15      0.11      0.12        56\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2215\n",
      "   macro avg       0.56      0.55      0.55      2215\n",
      "weighted avg       0.96      0.96      0.96      2215\n",
      "\n",
      "The model was 96.16% accuracte\n",
      "The model had a misclassification rate of 3.84%\n",
      "The model had a true positive rate of 10.71%\n",
      "The model had a false positive rate of 1.62%\n",
      "The model had a true negitive rate of 97.7%\n",
      "The model had a precision rate of 14.63%\n",
      "The model had a prevalence rate of 2.53%\n"
     ]
    }
   ],
   "source": [
    "dtree_results = pd.DataFrame(confusion_matrix(y_test, dtree_pred), columns=['pred_no_gun','pred_gun'],\n",
    "             index = ['actual_no_gun','actual_gun'])\n",
    "print(classification_report(y_test,dtree_pred))\n",
    "modelStats(dtree_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_no_gun</th>\n",
       "      <th>pred_gun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_no_gun</th>\n",
       "      <td>2124</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_gun</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred_no_gun  pred_gun\n",
       "actual_no_gun         2124        35\n",
       "actual_gun              50         6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model isnt that bad by its stats but again I feel like its basically just guessing no every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randf = RandomForestClassifier(n_estimators=150)\n",
    "randf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "randf_pred = randf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      2159\n",
      "           1       0.15      0.04      0.06        56\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2215\n",
      "   macro avg       0.56      0.52      0.52      2215\n",
      "weighted avg       0.95      0.97      0.96      2215\n",
      "\n",
      "The model was 97.07% accuracte\n",
      "The model had a misclassification rate of 2.93%\n",
      "The model had a true positive rate of 3.57%\n",
      "The model had a false positive rate of 0.51%\n",
      "The model had a true negitive rate of 97.55%\n",
      "The model had a precision rate of 15.38%\n",
      "The model had a prevalence rate of 2.53%\n"
     ]
    }
   ],
   "source": [
    "randf_results = pd.DataFrame(confusion_matrix(y_test, randf_pred), columns=['pred_no_gun','pred_gun'],\n",
    "             index = ['actual_no_gun','actual_gun'])\n",
    "print(classification_report(y_test,randf_pred))\n",
    "modelStats(randf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_no_gun</th>\n",
       "      <th>pred_gun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_no_gun</th>\n",
       "      <td>2148</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_gun</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pred_no_gun  pred_gun\n",
       "actual_no_gun         2148        11\n",
       "actual_gun              54         2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randf_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
